# The proposal
<!--
This is where the proposal should be outlined. 
-->

## Overview

The integrated lab  is a solution to centralise data management of the traditional unconnected lab (see "Integrated Lab" Fig \@ref(fig:intlab)). A first step in the realisation of an integrated lab would encompass a solution for collecting and harmonizing data streams from various lab instruments. The development of the R package *panacea* will attempt to provide a more universal solution for parsing unstructured (meta)data formats in a rectangular format---notably, separating variables, observations and values. This solution would therefore make analytical data more easily accessible for both humans and machines. In extension we intend that this solution centralises data management of labs by facilitating automatic data ingest (i.e., data import) as a subsystem of, e.g., the iRODS (Integrated Rule-Oriented Data System) [@Rajasekar2010; @Rajasekar2015]. Besides good data management, this tool has several other benefits:

1. New software updates of the vendor-supplied software that impact the output format can be more easily accommodated.
2. Data formats from defunct software and vendors can be more conveniently analysed and/or archived in a central data management system.
3. The integration of (meta)data from different sources can aid online monitoring of lab performance. Centralised data management could theoretically provide opportunities for early detection of problems, such as sample/reagent pollution and anomalous lab-environmental conditions. The latter problems would be much harder to detect with stand-alone vendor supplied solutions.


<!--
At a high-level address what your proposal is and how it will address the problem identified. Highlight any benefits to the R Community that follow from solving the problem. This should be your most compelling section.
-->


## Detail

Observational data generated by commercial analytical instrumentation and accompanying software are often recorded as unstructured (text) files ^[Note, that the methods proposed here still require a vendor-supplied electrical-to-digital signal conversion]. In this context we refer to "unstructured" as incorporating tables of data output intermingled with lines of, one ore more, variable-value-unit triplets. This lack of structure is perceivably less dramatic than that encountered for information entrained in emails and novels. Nonetheless, the primary task of identifying; variables, values, and units, as distinct entities as well as larger structures (e.g., tables), is the most challenging task in this undertaking.

At present we envision two possible solutions, which require varying degrees of human intervention.

1. A human-crafted (and adaptable) rule based system
2. A natural language processing (NLP) approach involving self-supervised machine learning

Both options would be preceded by a step entailing text normalization through tokenization. Tokenization will be performed with cascades of regular expressions for word (entity) delimiters.  These delimiters are likely not based on word boundaries but instead use a combination of punctuations and tabs as delimiters. On the other hand, special character and alphanumeric combinations, as occur in paths and dates, should constitute one token, and require special consideration. 

*Solution #1* would require writing a set of more-or-less universal rules that describe typical formatting structures of analytical instrument output. For example, frequencies of certain tokens in a collection of files can likely already help separate values from variables and units. This would roughly operate reversed to the tf-idf algorithm [@Jurafsky2021]---tokens with constant document frequencies are signposts for variables, whereas tokens representing values of a categorical variable might have varying document frequencies. A dictionary of SI units, and derivations thereof, could, in turn, filter units from variables. Whereas a set of rules based on sentence boundaries, punctuation, delimiters might help recognize larger structures (e.g., tables) that can help tie together the variables and their constituent variable values. 

*Solution #2* would be almost free of human intervention. This method could be reminiscent of part-of-speech tagging in order to recognise the individual entities of the triplet; variables, values, and units. Recognition of larger structures (i.e, tables) might be based on chunking approaches that reminisce the methods serving context free grammar and/or dependency grammar solutions in NLP.

Ultimately, the (meta)data tagging solution(s) will form the engine of the to be developed core function of *panacea*. This function for the read-out of the instrument data will then proceed with parsing of unstructured data into a more convenient human and machine-readable format. The user interface of the function will be modelled after readr [@readr] and vroom [@vroom].

Based on a a twofold reasoning, we propose encoding this solution in the C++ language. Firstly, we want to ensure compatibility with external data management software, notably the Integrated Rule-Oriented Data System (iRODS). In this use-case, the compiled C++ source code could be adapted to create a standardized protocol for ingestion into a central data management system. The second consideration is performance related, e.g., the demanding operation of tokenizing a large corpus. This approach of extending the R core interpreter with C++ ensures a lean and fast approach. In addition, the usage of the R package cpp11 [@cpp11] enables the Altrep framework for lazy load of data in R, ensuring further speed and convenience of the functionality.  



<!--
Go into more detail about the specifics of the project and it delivers against the problem.

Depending on project type the detail section should include:

 - [ ] Minimum Viable Product
 - [ ] Architecture
 - [ ] Assumptions
-->