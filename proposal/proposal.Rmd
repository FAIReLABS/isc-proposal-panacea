# The proposal
<!--
This is where the proposal should be outlined. 
-->

## Overview

The integrated lab  is a solution to centralise data management of the traditional unconnected lab (see "Integrated Lab" Fig \@ref(fig:intlab)). A first step in the realisation of an integrated lab would encompass a solution for collecting and harmonising data streams from various lab instruments. The development of the R package *panacea* will attempt to provide a more universal solution for parsing unstructured (meta)data formats in a rectangular format---notably, separating variables, units, and values. This solution would therefore make analytical data more easily accessible for both humans and machines. In extension we intend that this solution centralises data management of labs by facilitating automatic data ingest (i.e., data import) as a subsystem of, e.g., the iRODS (Integrated Rule-Oriented Data System) [@Rajasekar2010; @Rajasekar2015]. 

Besides addressing the vendor lock-in of analytical data and optimized data management solutions, this tool has several other benefits:

1. New software updates of the vendor-supplied software that impact the output format can be more easily accommodated, and do not require cumbersome updates of custom R solutions. 
2. Data formats from defunct software and vendors can be more conveniently analysed and/or archived in a central data management system.
3. The integration of (meta)data from different sources can aid online monitoring of lab performance. For example, centralised data management could theoretically provide opportunities for early detection of problems, such as sample/reagent pollution and anomalous lab-environmental conditions. The latter problems would be much harder to detect with stand-alone vendor supplied solutions.

To conclude, we want to put scientist back in control of their data, without having to rely on closed-sourced vendor software. This could save countless working hours and large sums of taxpayer money, which can then be spend on other tasks. Together with the benefits of integrated labs, this could lead to new innovations, more transparent science, and improve the inclusiveness of the academic community. 

<!--
At a high-level address what your proposal is and how it will address the problem identified. Highlight any benefits to the R Community that follow from solving the problem. This should be your most compelling section.
-->


## Detail {#sec:Detail}

Observational data generated by commercial analytical instrumentation and accompanying software is often recorded as unstructured (text) files ^[Note, that the methods proposed here still require a vendor-supplied electrical-to-digital signal conversion]. In this context we refer to "unstructured" as incorporating tab-delimited tables (Fig. \@ref(fig:input)) of data intermingled with lines of, one or more, variable-value-unit triplets (see line 1,3 and 4 of Fig. \@ref(fig:input)). 


```{r input, out.width="90%", fig.cap="An excerpt of how unstructured raw data files from analytical laboratory equipment typically looks like. This is an imaginary excerpt modelled after the main applicants experience with this type of data output. Note, that this is still a fairly structured data format in respect to what one can find in the wild.", echo=FALSE}
knitr::include_graphics("proposal/excerpt.png")
```


This lack of structure is perceivably less dramatic than that encountered for information entrained in emails and novels. Nonetheless, the primary task of identifying variables, values, and units, as distinct entities as well as larger structures (e.g., tables), is the most challenging task in this undertaking.

At present we envision two possible solutions, which require varying degrees of human intervention.

1. A human-crafted (and adaptable) rule based system.
2. A natural language processing (NLP) approach involving self-supervised machine learning.

Both options would be preceded by a step entailing text normalization through tokenization. Tokenization will be performed with cascades of regular expressions for word (entity) delimiters.  These delimiters will likely not be based on word boundaries, but instead use a combination of punctuations and tabs as delimiters. On the other hand, special character and alphanumeric combinations, as occur in paths and dates, should constitute one token, and require special consideration. 

***Solution #1*** would require writing a set of more-or-less universal rules that describe typical formatting structures of analytical instrument output. After preprocessing, we suspect that it is possible to generalise that all numeric tokens (strings) can be tagged as values. In turn, frequencies of the tokens in a collection of files can then help separate the remaining non-numeric values from the variables and units. This would roughly operate reversed to the tf-idf algorithm [@Jurafsky2021]---tokens with constant document frequencies are signposts for variables, whereas tokens representing values of a categorical variable might have varying document frequencies. A dictionary of SI units, and derivations thereof, could, in turn, filter units from variables. Finally, a set of rules based on sentence boundaries, punctuation, and delimiters might help recognize larger structures (e.g., tables) that can help tie together the variables and their constituent units and variable values. 

***Solution #2*** would be almost free of human intervention. This method could be reminiscent of part-of-speech tagging in order to recognise the individual entities of the triplet; variables, values, and units. Recognition of larger structures (i.e, tables) might be based on chunking approaches that reminisce the methods serving context free grammar and/or dependency grammar solutions in NLP [@Jurafsky2021].

Ultimately, the (meta)data tagging solution(s) will form the engine of the to-be-developed core function of *panacea*. This function for the read-out of the instrument data will then proceed with parsing of unstructured data into a more convenient human and machine-readable format. This output is preliminary envisioned to constitute a tibble [@tibble] with columns; variable (of type character), unit (of type character), relation (of type list), which constitutes a network of relations describing structures in the original document, and values (of type list) (see Table \@ref(tab:output)). The user-interface of the function will be modelled after readr [@readr] and vroom [@vroom].

```{r output, echo= FALSE}
set.seed(22)
xcp <- tibble::tibble(
  variable = c("Date Time", "Sample ID", "Peak Height Distribution", "EMHV", 
               "Position-x", "Position-y", "Position-z", "Time", "Count"),
  unit = c(NA_character_, NA_character_, "V", "mV", rep("um", 3), "s", 
           NA_character_),
  relation = list(
    c("file 1", "line 1", "section 1"),
    c("file 1", "line 1", "section 2"),
    c("file 1", "line 3", "section 1"),
    c("file 1", "line 3", "section 2"),
    c("file 1", "line 4", "section 1"),
    c("file 1", "line 4", "section 2"),
    c("file 1", "line 4", "section 3"),
    c("file 1", "table 1", "column 1"),
    c("file 1", "table 1", "column 2")
  ),
  values = list(
    as.character(lubridate::ymd_hm("2021-09-20  20:15")),
    "MON-233",
    210,
    2350,
    12,
    2,
    100,
    1:10,
    rpois(10, 60)
    )
  )  
knitr::kable(xcp, caption = "Provisional pancaea return value, based on the analytical data output of a virtual machine of Fig. \\ref{fig:input}.")
```


Based on a twofold reasoning, we propose encoding this solution in the C++ language. Firstly, we want to ensure compatibility with external data management software, notably iRODS. In this use-case, the compiled C++ source code could be adapted to create a standardized protocol for ingestion into a central data management system. The second consideration is performance related, e.g., the demanding operation of tokenizing a large corpus. This approach of extending the R core interpreter with C++ ensures a lean and fast approach. In addition, the usage of the R package cpp11 [@cpp11] enables the Altrep framework for lazy load of data in R, ensuring further speed and convenience of the functionality.  


<!--
Go into more detail about the specifics of the project and it delivers against the problem.

Depending on project type the detail section should include:

 - [ ] Minimum Viable Product
 - [ ] Architecture
 - [ ] Assumptions
-->
